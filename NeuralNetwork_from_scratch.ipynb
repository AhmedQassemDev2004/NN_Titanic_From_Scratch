{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation from Scratch: Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries:\n",
    "    - pandas: For data manipulation and analysis\n",
    "    - numpy: For numerical operations and array handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/titanic.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerid</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passengerid  survived  pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                name     sex   age  sibsp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   parch            ticket     fare cabin embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived    891\n",
       "pclass      891\n",
       "sex         891\n",
       "age         891\n",
       "sibsp       891\n",
       "parch       891\n",
       "fare        891\n",
       "embarked    891\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['name', 'passengerid', 'ticket', 'cabin'])\n",
    "df.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  sex_female  sex_male  \\\n",
       "0         0       3  22.0      1      0   7.2500       False      True   \n",
       "1         1       1  38.0      1      0  71.2833        True     False   \n",
       "2         1       3  26.0      0      0   7.9250        True     False   \n",
       "3         1       1  35.0      1      0  53.1000        True     False   \n",
       "4         0       3  35.0      0      0   8.0500       False      True   \n",
       "\n",
       "   embarked_C  embarked_Q  embarked_S  \n",
       "0       False       False        True  \n",
       "1        True       False       False  \n",
       "2       False       False        True  \n",
       "3       False       False        True  \n",
       "4       False       False        True  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked']= df['embarked'].fillna(df['embarked'].mode())\n",
    "df['age'] = df['age'].fillna(df['age'].mean())\n",
    "df = pd.get_dummies(df, columns=['sex', 'embarked'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['survived'].values.reshape(-1, 1)  \n",
    "X = df.drop(columns=['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.592481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.502445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.638789</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.786845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>-0.284663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.488854</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>0.407926</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.486337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>-0.369365</td>\n",
       "      <td>-0.207709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.386671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-0.823344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.044381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.176263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>-1.566107</td>\n",
       "      <td>-0.284663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.044381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0.827377</td>\n",
       "      <td>0.177063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.492378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pclass       age  sibsp  parch      fare  sex_female  sex_male  \\\n",
       "0    0.827377 -0.592481    1.0    0.0 -0.502445         0.0       1.0   \n",
       "1   -1.566107  0.638789    1.0    0.0  0.786845         1.0       0.0   \n",
       "2    0.827377 -0.284663    0.0    0.0 -0.488854         1.0       0.0   \n",
       "3   -1.566107  0.407926    1.0    0.0  0.420730         1.0       0.0   \n",
       "4    0.827377  0.407926    0.0    0.0 -0.486337         0.0       1.0   \n",
       "..        ...       ...    ...    ...       ...         ...       ...   \n",
       "886 -0.369365 -0.207709    0.0    0.0 -0.386671         0.0       1.0   \n",
       "887 -1.566107 -0.823344    0.0    0.0 -0.044381         1.0       0.0   \n",
       "888  0.827377  0.000000    1.0    2.0 -0.176263         1.0       0.0   \n",
       "889 -1.566107 -0.284663    0.0    0.0 -0.044381         0.0       1.0   \n",
       "890  0.827377  0.177063    0.0    0.0 -0.492378         0.0       1.0   \n",
       "\n",
       "     embarked_C  embarked_Q  embarked_S  \n",
       "0           0.0         0.0         1.0  \n",
       "1           1.0         0.0         0.0  \n",
       "2           0.0         0.0         1.0  \n",
       "3           0.0         0.0         1.0  \n",
       "4           0.0         0.0         1.0  \n",
       "..          ...         ...         ...  \n",
       "886         0.0         0.0         1.0  \n",
       "887         0.0         0.0         1.0  \n",
       "888         0.0         0.0         1.0  \n",
       "889         1.0         0.0         0.0  \n",
       "890         0.0         1.0         0.0  \n",
       "\n",
       "[891 rows x 10 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_cols = ['pclass', 'age', 'fare']\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "\n",
    "X = X.astype(float)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Neural Network Parameters\n",
    "\n",
    "This function initializes the weights and biases for a 4-layer neural network using He initialization.\n",
    "The architecture is:\n",
    "- Input layer: input_size neurons\n",
    "- Hidden layer 1: 64 neurons with ReLU activation\n",
    "- Hidden layer 2: 32 neurons with ReLU activation  \n",
    "- Hidden layer 3: 16 neurons with ReLU activation\n",
    "- Output layer: 1 neuron with sigmoid activation\n",
    "\n",
    "Parameters:\n",
    "  input_size: Number of features in the input layer\n",
    "\n",
    "Returns:\n",
    "  params: Dictionary containing:\n",
    "    - W1, W2, W3, W4: Weight matrices for each layer\n",
    "    - b1, b2, b3, b4: Bias vectors for each layer\n",
    "    - m_* and v_*: Momentum and velocity terms for Adam optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(input_size):\n",
    "    params = {}\n",
    "    params['W1'] = np.random.randn(input_size, 64) * np.sqrt(1 / input_size)\n",
    "    params['W2'] = np.random.randn(64, 32) * np.sqrt(1 / 64)\n",
    "    params['W3'] = np.random.randn(32, 16) * np.sqrt(1 / 32)\n",
    "    params['W4'] = np.random.randn(16, 1) * np.sqrt(1 / 16)\n",
    "    \n",
    "    params['b1'] = np.zeros((1, 64))\n",
    "    params['b2'] = np.zeros((1, 32))\n",
    "    params['b3'] = np.zeros((1, 16))\n",
    "    params['b4'] = np.zeros((1, 1))\n",
    "\n",
    "    for key in ['W1', 'W2', 'W3', 'W4', 'b1', 'b2', 'b3', 'b4']:\n",
    "        params[f'm_{key}'] = np.zeros_like(params[key])\n",
    "        params[f'v_{key}'] = np.zeros_like(params[key])\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation Implementation\n",
    "\n",
    "This section implements the forward propagation for our 4-layer neural network.\n",
    "The forward pass consists of:\n",
    "1. Linear transformation + ReLU activation for layers 1-2\n",
    "2. Linear transformation + Sigmoid activation for output layer\n",
    "3. Optional dropout regularization after layers 1-2\n",
    "\n",
    "The architecture processes data through:\n",
    "- Input layer -> Hidden layer 1 (ReLU) -> Hidden layer 2 (ReLU) -> \n",
    "  Hidden layer 3 (ReLU) -> Output layer (Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(1, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    Z = np.asarray(Z, dtype=np.float64)  # Ensure Z is a NumPy array with float64 dtype\n",
    "    Z = np.clip(Z, -500, 500)  # Adjust these limits as needed\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def forward_prop(X, params, keep_prob=1.0):\n",
    "    cache = {} \n",
    "\n",
    "    Z1 = np.dot(X, params['W1']) + params['b1']\n",
    "    A1 = ReLU(Z1)\n",
    "\n",
    "    if keep_prob < 1.0:\n",
    "        D1 = np.random.randn(*A1.shape) < keep_prob\n",
    "        A1 *= D1 \n",
    "        A1 /= keep_prob\n",
    "        cache['D1'] = D1\n",
    "    \n",
    "    Z2 = np.dot(A1, params['W2']) + params['b2']\n",
    "    A2 = ReLU(Z2)\n",
    "\n",
    "    if keep_prob < 1.0:\n",
    "        D2 = np.random.rand(*A2.shape) < keep_prob\n",
    "        A2 *= D2\n",
    "        A2 /= keep_prob\n",
    "        cache['D2'] = D2\n",
    "\n",
    "    Z3 = np.dot(A2, params['W3']) + params['b3']\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "\n",
    "    cache['Z1'], cache['A1'], cache['Z2'], cache['A2'], cache['Z3'], cache['A3'] = Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating mini-batches for training involves:\n",
    "1. Randomly shuffling the training data to avoid order bias\n",
    "2. Splitting data into smaller batches of fixed size (e.g. 32 samples)\n",
    "3. Handling any remaining samples that don't fit evenly into batches\n",
    "4. Returning list of (X_batch, y_batch) tuples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(X, y, batch_size=32):\n",
    "    batches = []\n",
    "    m = X.shape[0]  # Number of samples\n",
    "\n",
    "    permutation = np.random.permutation(m)  # Shuffle indices\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    X_shuffled = X[permutation]\n",
    "    y_shuffled = y[permutation]  \n",
    "\n",
    "    num_batches = m // batch_size\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        X_mini = X_shuffled[i * batch_size:(i + 1) * batch_size]\n",
    "        y_mini = y_shuffled[i * batch_size:(i + 1) * batch_size]\n",
    "        batches.append((X_mini, y_mini))\n",
    "\n",
    "    if m % batch_size != 0:  # Add remaining samples if any\n",
    "        X_mini = X_shuffled[num_batches * batch_size:]\n",
    "        y_mini = y_shuffled[num_batches * batch_size:]\n",
    "        batches.append((X_mini, y_mini))\n",
    "\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Loss\n",
    "\n",
    "Step 1: Cross-Entropy Loss\n",
    "m = number of samples\n",
    "Formula: CE = 1/m * Σ(-y*log(ŷ + ε) + (1-y)*log(1-ŷ + ε))\n",
    "Where:\n",
    "- ŷ is predicted value (y_pred)\n",
    "- y is true label\n",
    "- ε is small constant (1e-10) to prevent log(0)\n",
    "\n",
    "Step 2: L2 Regularization Loss\n",
    "Formula: L2 = λ * (ΣW1² + ΣW2² + ΣW3²)\n",
    "Where:\n",
    "- λ is regularization strength (lambda_reg)\n",
    "- W1, W2, W3 are weight matrices\n",
    "\n",
    "Step 3: Total Loss\n",
    "Formula: Total Loss = Cross-Entropy Loss + L2 Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y, params, lambda_reg=0):\n",
    "    m = y.shape[0]\n",
    "    cross_entropy_loss = 1/m * np.sum(-y*np.log(y_pred+1e-10)+(1-y)*np.log(1-y_pred+1e-10))\n",
    "    l2_loss = lambda_reg * (np.sum(params['W1']**2) + np.sum(params['W2']**2) + np.sum(params['W3']**2))\n",
    "    return cross_entropy_loss + l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation\n",
    "\n",
    "Purpose: Calculate gradients for updating network parameters during training\n",
    "\n",
    "Input Parameters:\n",
    "- X: Input features\n",
    "- y: True labels \n",
    "- params: Dictionary containing weights (W1,W2,W3) and biases (b1,b2,b3)\n",
    "- cache: Dictionary containing activations (A1,A2,A3) and pre-activations (Z1,Z2,Z3)\n",
    "- keep_prob: Dropout probability (1.0 means no dropout)\n",
    "\n",
    "Process:\n",
    "1. Calculate output layer gradients (dZ3,dW3,db3)\n",
    "2. Calculate hidden layer 2 gradients (dA2,dZ2,dW2,db2) with ReLU derivative\n",
    "3. Apply dropout if keep_prob < 1.0\n",
    "4. Calculate hidden layer 1 gradients (dA1,dZ1,dW1,db1) with ReLU derivative\n",
    "5. Apply dropout if keep_prob < 1.0\n",
    "\n",
    "Returns:\n",
    "- grads: Dictionary containing gradients for all weights and biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(X, y, params, cache, keep_prob=1.0):\n",
    "   \n",
    "    m = X.shape[0]  # Number of samples\n",
    "    grads = {}\n",
    "\n",
    "    A1, A2, A3 = cache['A1'], cache['A2'], cache['A3']\n",
    "    Z1, Z2, Z3 = cache['Z1'], cache['Z2'], cache['Z3']\n",
    "\n",
    "    dZ3 = A3 - y  # Derivative of binary cross-entropy loss w.r.t Z3\n",
    "    dW3 = (1/m) * np.dot(A2.T, dZ3)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=0, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(dZ3, params['W3'].T)\n",
    "    dZ2 = dA2 * (Z2 > 0)  # ReLU derivative\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    if keep_prob < 1.0:\n",
    "        dA2 *= cache['D2']\n",
    "        dA2 /= keep_prob\n",
    "\n",
    "    dA1 = np.dot(dZ2, params['W2'].T)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # ReLU derivative\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    if keep_prob < 1.0:\n",
    "        dA1 *= cache['D1']\n",
    "        dA1 /= keep_prob\n",
    "\n",
    "    grads['dW1'], grads['db1'] = dW1, db1\n",
    "    grads['dW2'], grads['db2'] = dW2, db2\n",
    "    grads['dW3'], grads['db3'] = dW3, db3\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually update params after back prop grads calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "    updated_params = params.copy()  # Create a copy of params to avoid modifying the original\n",
    "\n",
    "    # Update first and second moments for weights and biases\n",
    "    for key in ['W1', 'W2', 'W3', 'b1', 'b2', 'b3']:\n",
    "        # Update first moment (momentum)\n",
    "        updated_params[f'm_{key}'] = beta1 * params[f'm_{key}'] + (1 - beta1) * grads[f'd{key}']\n",
    "        \n",
    "        # Update second moment (RMSprop)\n",
    "        updated_params[f'v_{key}'] = beta2 * params[f'v_{key}'] + (1 - beta2) * (grads[f'd{key}'] ** 2)\n",
    "        \n",
    "        # Bias correction for first and second moments\n",
    "        m_hat = updated_params[f'm_{key}'] / (1 - beta1 ** t)\n",
    "        v_hat = updated_params[f'v_{key}'] / (1 - beta2 ** t)\n",
    "        \n",
    "        # Update parameters\n",
    "        updated_params[key] = params[key] - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the main training model that will use the forward prop, back prop and parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, params, epochs, decay_rate=0.2, learning_rate=0.01, batch_size=32, keep_prob=0.8, lambda_reg=0, beta1=0.09, beta2=0.999):\n",
    "    learning_rate0 = learning_rate\n",
    "    t = 1  # Initialize time step for Adam\n",
    "\n",
    "    for i in range(epochs):\n",
    "        batches = create_mini_batches(X, y, batch_size=batch_size)\n",
    "        epoch_loss = 0  \n",
    "\n",
    "        for X_mini, y_mini in batches:\n",
    "            A3, cache = forward_prop(X_mini, params, keep_prob)\n",
    "            loss = compute_loss(A3, y_mini, params, lambda_reg=lambda_reg)\n",
    "            epoch_loss += loss  \n",
    "            grads = back_prop(X_mini, y_mini, params, cache, keep_prob)\n",
    "            params = update_params(params, grads, learning_rate, beta1, beta2, t)\n",
    "            t += 1  # Increment time step\n",
    "        \n",
    "        learning_rate = (1 / (1 + decay_rate * i)) * learning_rate0\n",
    "        epoch_loss /= len(batches)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch {i + 1} Done\")\n",
    "            print(\"Learning rate: \", learning_rate)\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = forward_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning\n",
    "\n",
    "In the next cell, we perform hyperparameter tuning using random search:\n",
    "\n",
    "1. First, we split our training data into training and validation sets (80-20 split)\n",
    "\n",
    "2. We define a parameter grid with different values to try for:\n",
    "   - learning_rate: Controls step size during optimization (0.001, 0.01, 0.1)\n",
    "   - keep_prob: Dropout probability (0.7, 0.8, 0.9) \n",
    "   - beta1: First momentum parameter for Adam optimizer (0.9, 0.95)\n",
    "   - beta2: Second momentum parameter for Adam optimizer (0.999, 0.995)\n",
    "   - decay_rate: Learning rate decay (0.0, 0.001, 0.01)\n",
    "   - batch_size: Mini-batch sizes (32, 64, 128)\n",
    "\n",
    "3. Using sklearn's ParameterSampler, we randomly sample 10 different combinations\n",
    "   of these parameters to try\n",
    "\n",
    "4. For each parameter combination:\n",
    "   - Train the model for 2000 epochs\n",
    "   - Evaluate accuracy on validation set\n",
    "   - Keep track of best performing parameters\n",
    "\n",
    "This helps us find optimal hyperparameters without having to try every possible combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing params:  {'learning_rate': 0.1, 'lambda_reg': 0, 'keep_prob': 0.8, 'decay_rate': 0.001, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.09099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.08340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.07698229407236336\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.07147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.066711140760507\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.06253908692933084\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.05885815185403179\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.05558643690939411\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.0526592943654555\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.05002501250625312\n",
      "Validation Accuracy: 80.86%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0.01, 'keep_prob': 0.8, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.9, 'batch_size': 32}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.005025125628140704\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0033444816053511705\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.002004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0014306151645207437\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0012515644555694619\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0009099181073703366\n",
      "Validation Accuracy: 60.88%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0, 'keep_prob': 0.9, 'decay_rate': 0.001, 'beta2': 0.999, 'beta1': 0.95, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0009099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0008340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0007698229407236335\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.0007147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0006671114076050701\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0006253908692933083\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0005885815185403178\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0005558643690939412\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.000526592943654555\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0005002501250625312\n",
      "Validation Accuracy: 59.75%\n",
      "Testing params:  {'learning_rate': 0.1, 'lambda_reg': 0.01, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.05025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.02506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.02004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.01669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.01430615164520744\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.01251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.011123470522803115\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.01001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.009099181073703366\n",
      "Validation Accuracy: 77.14%\n",
      "Testing params:  {'learning_rate': 0.1, 'lambda_reg': 0.001, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.95, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.05025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.02506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.02004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.01669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.01430615164520744\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.01251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.011123470522803115\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.01001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.009099181073703366\n",
      "Validation Accuracy: 79.68%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0.001, 'keep_prob': 0.9, 'decay_rate': 0.001, 'beta2': 0.999, 'beta1': 0.95, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.009099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.008340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.007698229407236336\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.007147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0066711140760507\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.006253908692933083\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.005885815185403178\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.005558643690939411\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.0052659294365455505\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.005002501250625312\n",
      "Validation Accuracy: 60.93%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0.01, 'keep_prob': 0.7, 'decay_rate': 0.001, 'beta2': 0.995, 'beta1': 0.95, 'batch_size': 32}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.009099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.008340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.007698229407236336\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.007147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0066711140760507\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.006253908692933083\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.005885815185403178\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.005558643690939411\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.0052659294365455505\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.005002501250625312\n",
      "Validation Accuracy: 60.84%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0, 'keep_prob': 0.8, 'decay_rate': 0.001, 'beta2': 0.995, 'beta1': 0.95, 'batch_size': 32}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.009099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.008340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.007698229407236336\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.007147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0066711140760507\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.006253908692933083\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.005885815185403178\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.005558643690939411\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.0052659294365455505\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.005002501250625312\n",
      "Validation Accuracy: 60.88%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0.001, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.95, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.005025125628140704\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0033444816053511705\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.002004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0014306151645207437\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0012515644555694619\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0009099181073703366\n",
      "Validation Accuracy: 60.88%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0.01, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.95, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.005025125628140704\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0033444816053511705\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.002004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0014306151645207437\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0012515644555694619\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0009099181073703366\n",
      "Validation Accuracy: 60.88%\n",
      "Testing params:  {'learning_rate': 0.1, 'lambda_reg': 0.001, 'keep_prob': 0.8, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.95, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.05025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.02506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.02004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.01669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.01430615164520744\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.01251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.011123470522803115\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.01001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.009099181073703366\n",
      "Validation Accuracy: 60.84%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0.001, 'keep_prob': 0.7, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.95, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0005025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.00033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.00020040080160320639\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.00014306151645207438\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0001251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.00011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.00010010010010010009\n",
      "Epoch 1000 Done\n",
      "Learning rate:  9.099181073703367e-05\n",
      "Validation Accuracy: 59.66%\n",
      "Testing params:  {'learning_rate': 0.1, 'lambda_reg': 0.01, 'keep_prob': 0.8, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.05025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.02506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.02004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.01669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.01430615164520744\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.01251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.011123470522803115\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.01001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.009099181073703366\n",
      "Validation Accuracy: 74.34%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.005025125628140704\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0033444816053511705\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.002004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0014306151645207437\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0012515644555694619\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0009099181073703366\n",
      "Validation Accuracy: 60.88%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0.001, 'keep_prob': 0.8, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.9, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0005025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.00033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.00020040080160320639\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.00014306151645207438\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0001251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.00011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.00010010010010010009\n",
      "Epoch 1000 Done\n",
      "Learning rate:  9.099181073703367e-05\n",
      "Validation Accuracy: 55.59%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0.01, 'keep_prob': 0.7, 'decay_rate': 0.0, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 64}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.001\n",
      "Validation Accuracy: 59.31%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.999, 'beta1': 0.9, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0005025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.00033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.00020040080160320639\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.00014306151645207438\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0001251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.00011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.00010010010010010009\n",
      "Epoch 1000 Done\n",
      "Learning rate:  9.099181073703367e-05\n",
      "Validation Accuracy: 56.29%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0.001, 'keep_prob': 0.7, 'decay_rate': 0.001, 'beta2': 0.999, 'beta1': 0.9, 'batch_size': 32}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0009099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0008340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0007698229407236335\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.0007147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0006671114076050701\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0006253908692933083\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0005885815185403178\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0005558643690939412\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.000526592943654555\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0005002501250625312\n",
      "Validation Accuracy: 59.05%\n",
      "Testing params:  {'learning_rate': 0.001, 'lambda_reg': 0, 'keep_prob': 0.9, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.9, 'batch_size': 128}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.0005025125628140703\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.00033444816053511704\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.0002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.00020040080160320639\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.0001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.00014306151645207438\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0001251564455569462\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.00011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.00010010010010010009\n",
      "Epoch 1000 Done\n",
      "Learning rate:  9.099181073703367e-05\n",
      "Validation Accuracy: 56.42%\n",
      "Testing params:  {'learning_rate': 0.01, 'lambda_reg': 0, 'keep_prob': 0.8, 'decay_rate': 0.01, 'beta2': 0.995, 'beta1': 0.95, 'batch_size': 32}\n",
      "Epoch 100 Done\n",
      "Learning rate:  0.005025125628140704\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.0033444816053511705\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.002506265664160401\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.002004008016032064\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.001669449081803005\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.0014306151645207437\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.0012515644555694619\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.0011123470522803114\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.001001001001001001\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.0009099181073703366\n",
      "Validation Accuracy: 60.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler, train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'keep_prob': [0.7, 0.8, 0.9],\n",
    "    'beta1': [0.9, 0.95],\n",
    "    'beta2': [0.999, 0.995],\n",
    "    'decay_rate': [0.0, 0.001, 0.01],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'lambda_reg': [0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "\n",
    "# Generates 20 random samples of the hyper params\n",
    "param_samples = list(ParameterSampler(param_grid, n_iter=20, random_state=42))\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "params = init_params(X.shape[1])\n",
    "\n",
    "for params_sample in param_samples:\n",
    "    print(\"Testing params: \", params_sample)\n",
    "\n",
    "    model_params = model(\n",
    "        X_train, y_train,\n",
    "        params=params,\n",
    "        epochs=1000,\n",
    "        learning_rate=params_sample['learning_rate'],\n",
    "        keep_prob=params_sample['keep_prob'],\n",
    "        beta1=params_sample['beta1'],\n",
    "        beta2=params_sample['beta2'],\n",
    "        decay_rate=params_sample['decay_rate']\n",
    "    )\n",
    "\n",
    "    y_pred, _ = predict(X_val, model_params)\n",
    "    accuracy = np.mean((y_pred > 0.5) == y_val) * 100\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Track best params\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we got the **best_params** let's train the model with 5000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Done\n",
      "Learning rate:  0.09099181073703368\n",
      "Epoch 200 Done\n",
      "Learning rate:  0.08340283569641367\n",
      "Epoch 300 Done\n",
      "Learning rate:  0.07698229407236336\n",
      "Epoch 400 Done\n",
      "Learning rate:  0.07147962830593281\n",
      "Epoch 500 Done\n",
      "Learning rate:  0.066711140760507\n",
      "Epoch 600 Done\n",
      "Learning rate:  0.06253908692933084\n",
      "Epoch 700 Done\n",
      "Learning rate:  0.05885815185403179\n",
      "Epoch 800 Done\n",
      "Learning rate:  0.05558643690939411\n",
      "Epoch 900 Done\n",
      "Learning rate:  0.0526592943654555\n",
      "Epoch 1000 Done\n",
      "Learning rate:  0.05002501250625312\n",
      "Epoch 1100 Done\n",
      "Learning rate:  0.04764173415912339\n",
      "Epoch 1200 Done\n",
      "Learning rate:  0.04547521600727604\n",
      "Epoch 1300 Done\n",
      "Learning rate:  0.04349717268377556\n",
      "Epoch 1400 Done\n",
      "Learning rate:  0.041684035014589414\n",
      "Epoch 1500 Done\n",
      "Learning rate:  0.040016006402561026\n",
      "Epoch 1600 Done\n",
      "Learning rate:  0.038476337052712584\n",
      "Epoch 1700 Done\n",
      "Learning rate:  0.037050759540570584\n",
      "Epoch 1800 Done\n",
      "Learning rate:  0.03572704537334763\n",
      "Epoch 1900 Done\n",
      "Learning rate:  0.03449465332873405\n",
      "Epoch 2000 Done\n",
      "Learning rate:  0.03334444814938313\n",
      "Epoch 2100 Done\n",
      "Learning rate:  0.032268473701193935\n",
      "Epoch 2200 Done\n",
      "Learning rate:  0.03125976867771179\n",
      "Epoch 2300 Done\n",
      "Learning rate:  0.030312215822976663\n",
      "Epoch 2400 Done\n",
      "Learning rate:  0.029420417769932334\n",
      "Epoch 2500 Done\n",
      "Learning rate:  0.02857959416976279\n",
      "Epoch 2600 Done\n",
      "Learning rate:  0.02778549597110308\n",
      "Epoch 2700 Done\n",
      "Learning rate:  0.027034333603676672\n",
      "Epoch 2800 Done\n",
      "Learning rate:  0.026322716504343247\n",
      "Epoch 2900 Done\n",
      "Learning rate:  0.02564760194921775\n",
      "Epoch 3000 Done\n",
      "Learning rate:  0.025006251562890727\n",
      "Epoch 3100 Done\n",
      "Learning rate:  0.024396194193705784\n",
      "Epoch 3200 Done\n",
      "Learning rate:  0.023815194093831867\n",
      "Epoch 3300 Done\n",
      "Learning rate:  0.023261223540358228\n",
      "Epoch 3400 Done\n",
      "Learning rate:  0.022732439190725165\n",
      "Epoch 3500 Done\n",
      "Learning rate:  0.02222716159146477\n",
      "Epoch 3600 Done\n",
      "Learning rate:  0.021743857360295715\n",
      "Epoch 3700 Done\n",
      "Learning rate:  0.02128112364332837\n",
      "Epoch 3800 Done\n",
      "Learning rate:  0.02083767451552407\n",
      "Epoch 3900 Done\n",
      "Learning rate:  0.020412329046744237\n",
      "Epoch 4000 Done\n",
      "Learning rate:  0.02000400080016003\n",
      "Epoch 4100 Done\n",
      "Learning rate:  0.019611688566385566\n",
      "Epoch 4200 Done\n",
      "Learning rate:  0.019234468166955187\n",
      "Epoch 4300 Done\n",
      "Learning rate:  0.018871485185884128\n",
      "Epoch 4400 Done\n",
      "Learning rate:  0.018521948508983144\n",
      "Epoch 4500 Done\n",
      "Learning rate:  0.018185124568103294\n",
      "Epoch 4600 Done\n",
      "Learning rate:  0.01786033220217896\n",
      "Epoch 4700 Done\n",
      "Learning rate:  0.01754693805930865\n",
      "Epoch 4800 Done\n",
      "Learning rate:  0.01724435247456458\n",
      "Epoch 4900 Done\n",
      "Learning rate:  0.016952025767079167\n",
      "Epoch 5000 Done\n",
      "Learning rate:  0.016669444907484583\n"
     ]
    }
   ],
   "source": [
    "params = init_params(X.shape[1])\n",
    "params = model(X_train, y_train, params, 5000, **best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the model's performance on both training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 63.66%\n"
     ]
    }
   ],
   "source": [
    "y_pred_train, _ = predict(X_train, params)\n",
    "train_accuracy = np.mean((y_pred_train > 0.5) == y_train) * 100\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 59.57%\n"
     ]
    }
   ],
   "source": [
    "y_pred, cahce = predict(X_test, params)\n",
    "\n",
    "accuracy = np.mean((y_pred > 0.5) == y_test) *100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
